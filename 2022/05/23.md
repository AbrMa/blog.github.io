## 2022 05 23

## Table of contents

+ [Weekly Essay](#weekly-essay)

+ [How does a DBMS parse a SQL query?](#how-does-a-dbms-parse-a-sql-query)

### Weekly Essay

I started this week by reading some articles on the hub about all the resources available for learning. I found the o'reilly platform very useful for the question assignment, I read about 3 chapters from the book "flex & bison" by John Levine, in which he describes with the right amount of detail how those two pieces of software work together for the lexical analyzer and the semantic analyzer in a compiler.

I refreshed my JS knowledge by practicing some exercises that involved concepts like arrow functions (which I always found a confusing concept) and destructuring. I did this with the objective of having a better understanding of the react framework.

I read a part of the PostgreSQL source code, I liked the way the project was structured, because it was somewhat easy to find the [files](https://github.com/postgres/postgres/tree/master/src/backend/parser) that I was most interested in.

I think I focused too much in the research of the question and overanalyzing certain aspects of it.

I did underestimate the task of the contribution to the open source project, I dedicated a lot of time in trying to find interesting projects in python and c++ which are the languages I feel most comfortable with, I overlooked the complexity of building those projects. I had a special interest in the [sympy
](https://github.com/sympy/sympy) project, I started by reading documentation related to the software itself, how to use it, how to contribute to the project, the code of conduct and how to properly build and generate a pull request to the project, but all the time invested was not worth it, because I had some troubles while building the dev environment. I think that it was a good idea to not directly ask for the assignment of an issue, because I would not have been able to fix it, and it would have been a waste of time for both myself and the maintainers.

I'm overall worried because I haven't been able to choose a project to start working, and I noticed that work that depends on many parts can be a lot more complicated, but that's something I'll have to deal with if I wanna become a good developer. I will need to find a better strategy for solving my current issue, the problem is that at the time of writing I can't come with any solution, I will have to use the technique from the "Learning how to learn" course and try to enter diffuse mode hoping to both relax and find a more suitable solution.

### How does a DBMS parse a SQL query?

To answer this question first we need a little bit of context.

#### Scanning and Parsing

Parsing is the process of analyzing a string of symbols conforming to the rules of a formal grammar.

In order to do that first we need to analyze the stream of characters or symbols and group them together into meaningful chunks of information called lexemes. This is normally the job of a program called *scanner*.

Lets say for example that we are analyzing the next expression

```
operation = 1 * 2 + 3 * 4 + 5
```

Then the scanner will produce as an output the next sequence of tokens

``` 
<id,1> <=> <1> <*> <2> <+> <3> <*> <4> <+> <5>
```

Then the parser's job is to figure out the relationship between those tokens, usually it's represented as a parse tree.

```    
       =
      / \
     +  <id,1>
    / \
   +   5
  / \
 *   *
/ \ / \
1 2 3 4 
```
#### Flex and Bison

Flex is a program that generates programs that perform pattern-matching on text, while bison is a parser generator.

The way flex works is that it reads a file and finds patters, then depending on the pattern it executes an action.

A simple flex program looks like this

```
/* recognize tokens for the calculator and print them out */
%{
 /*Declarations and settings*/
 enum yytokentype {
  NUMBER = 258,
   ADD = 259,
   SUB = 260,
   MUL = 261,
   DIV = 262,
   ABS = 263,
   EOL = 264
  };
 int yylval;
%}
/*Patterns and actions*/
%%
"+" { return ADD; }
"-" { return SUB; }
"*" { return MUL; }
"/" { return DIV; }
"|" { return ABS; }
[0-9]+ { yylval = atoi(yytext); return NUMBER; }
\n { return EOL; }
[ \t] { /* ignore whitespace */ }
. { printf("Mystery character %c\n", *yytext); }
%%
/*C code*/
main(int argc, char **argv) {
 int tok;
 while(tok = yylex()) {
 printf("%d", tok);
 if(tok == NUMBER) printf(" = %d\n", yylval);
 else printf("\n");
 }
}
```
Usually the scanner is used as a subroutine, because you can make it return tokens, and then those tokens can be handled by the parser, however the parser needs to know how to handle tokens so that it builds the parse tree, the way we do that is by describing rules of the grammar with a simplified version of [BNF Grammars](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form)


A simple bison file looks like this

```
/* simplest version of calculator */
%{
#include <stdio.h>
%}
/* Declarations */
%token NUMBER
%token ADD SUB MUL DIV ABS
%token EOL
/*Grammar Rules*/
%%
calclist: /* nothing */ matches at beginning of input
 | calclist exp EOL { printf("= %d\n", $1); } EOL is end of an expression
 ;
exp: factor default $$ = $1
 | exp ADD factor { $$ = $1 + $3; }
 | exp SUB factor { $$ = $1 - $3; }
 ;
factor: term default $$ = $1
 | factor MUL term { $$ = $1 * $3; }
 | factor DIV term { $$ = $1 / $3; }
 ;
term: NUMBER default $$ = $1
 | ABS term { $$ = $2 >= 0? $2 : - $2; }
;
%%
/*C code*/
main(int argc, char **argv)
{
 yyparse();
}
yyerror(char *s)
{
 fprintf(stderr, "error: %s\n", s);
}
````
Bison makes the parsing automatically and it can create data structures for later use. It's useful to know that bison doesn't know how to handle ambiguous grammars, so we must provide an unambiguous one, or remove its ambiguity.

The main advantage of using flex & bison together over an ad-hoc solution is that its faster and easier to debug. 

### The SQL part

The Structured Query Language is used to manage [relational databases](https://www.oracle.com/database/what-is-a-relational-database/). In the particular case of [PostgreSQL](https://github.com/postgres/postgres/tree/master/src/backend/parser) there are three main files that take care of the parsing part.

```
parser.c	things start here
scan.l		break query into tokens
gram.y		parse the tokens and produce a "raw" parse tree
```

The [parser.c](https://github.com/postgres/postgres/blob/master/src/backend/parser/parser.c) file initializes the flex scanner as a coroutine 

```
yyscanner = scanner_init(str, &yyextra.core_yy_extra,
			&ScanKeywords, ScanKeywordTokens);
```

Inside the [scan.l](https://github.com/postgres/postgres/blob/master/src/backend/parser/scan.l) file we can find the basic structure that was described before, it contains declarations and settings. It's very interesting to see the declaration of some regular expressions to simplify code later such as

```
digit			[0-9]
integer			{digit}+
```

Then in the second part we find the usual regex followed by an action format

```
{colon_equals} {
	         SET_YYLLOC(); 
                 /*Returns a token*/
		 return COLON_EQUALS;
               }
```

Some patterns execute other functions instead of returning a specific token, for example ```{real_junk}``` handles an error.

```
{real_junk} {
              SET_YYLLOC();
              yyerror("trailing junk after numeric literal");
            }
```

After the scanning process is over, returning back to the [parser.c](https://github.com/postgres/postgres/blob/master/src/backend/parser/parser.c) file we can see that if everything was successful the parse three will be generated. 

```
/* initialize the bison parser */
parser_init(&yyextra);

/* Parse! */
yyresult = base_yyparse(yyscanner);

/* Clean up (release memory) */
scanner_finish(yyscanner);

if (yyresult)				/* error */
	return NIL;

return yyextra.parsetree;
```

With a little bit of more detail if we observe the [gram.y](https://github.com/postgres/postgres/blob/master/src/backend/parser/gram.y) file we can see again that at the top we can find definitions, structs and function declarations, 

```
#define YYMALLOC palloc
#define YYFREE   pfree
```

```
/* Private struct for the result of import_qualification production */
typedef struct ImportQual
{
	ImportForeignSchemaType type;
	List	   *table_names;
} ImportQual;
```

```
/* Private struct for the result of opt_select_limit production */
typedef struct SelectLimit
{
	Node	   *limitOffset;
	Node	   *limitCount;
	LimitOption limitOption;
} SelectLimit;
```

Then there are multiple grammar productions such as the one to handle the ```DROP TABLE``` statement 

```
/*****************************************************************************
 *
 *		QUERY :
 *				DROP TABLESPACE <tablespace>
 *
 *		No need for drop behaviour as we cannot implement dependencies for
 *		objects in other databases; we can only support RESTRICT.
 *
 ****************************************************************************/

DropTableSpaceStmt: DROP TABLESPACE name
				{
					DropTableSpaceStmt *n = makeNode(DropTableSpaceStmt);

					n->tablespacename = $3;
					n->missing_ok = false;
					$$ = (Node *) n;
				}
				|  DROP TABLESPACE IF_P EXISTS name
				{
					DropTableSpaceStmt *n = makeNode(DropTableSpaceStmt);

					n->tablespacename = $5;
					n->missing_ok = true;
					$$ = (Node *) n;
				}
		;

```

And finally at the end of the file we can find C code with mostly auxiliary functions such as the ```parser_init``` function

```
/* parser_init()
 * Initialize to parse one query string
 */
void
parser_init(base_yy_extra_type *yyext)
{
	yyext->parsetree = NIL;		/* in case grammar forgets to set it */
}
```